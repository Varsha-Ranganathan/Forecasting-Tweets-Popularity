{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8169760,"sourceType":"datasetVersion","datasetId":4759473}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GAN Generative Model","metadata":{}},{"cell_type":"markdown","source":"### Install Necessary Libraries and Define Functions, Importing the models from Encoder+LSTM Architecture","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:39:20.685059Z","iopub.execute_input":"2024-04-22T18:39:20.685699Z","iopub.status.idle":"2024-04-22T18:39:34.695317Z","shell.execute_reply.started":"2024-04-22T18:39:20.685670Z","shell.execute_reply":"2024-04-22T18:39:34.694332Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\nfrom torch.nn.functional import softmax, log_softmax\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom tqdm import tqdm\nimport warnings\nfrom sentence_transformers import SentenceTransformer, util","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T18:39:38.363487Z","iopub.execute_input":"2024-04-22T18:39:38.363851Z","iopub.status.idle":"2024-04-22T18:39:48.360170Z","shell.execute_reply.started":"2024-04-22T18:39:38.363821Z","shell.execute_reply":"2024-04-22T18:39:48.359123Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r'http\\S+', '[URL]', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+', '[MENTION]', text)\n    text.replace(\"\\n\",\"\")\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:39:52.984024Z","iopub.execute_input":"2024-04-22T18:39:52.985086Z","iopub.status.idle":"2024-04-22T18:39:52.990056Z","shell.execute_reply.started":"2024-04-22T18:39:52.985038Z","shell.execute_reply":"2024-04-22T18:39:52.989023Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\nb_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nb_max_length = 128\n\ndef preprocess_text(text):\n    text = re.sub(r'http\\S+', '[URL]', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+', '[MENTION]', text)\n    \n    tokens = b_tokenizer(text, max_length=b_max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n    return tokens['input_ids'], tokens['attention_mask']\n\nspecial_tokens_dict = {'additional_special_tokens': ['[URL]', '[MENTION]']}\nnum_added_toks = b_tokenizer.add_special_tokens(special_tokens_dict)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:39:58.111401Z","iopub.execute_input":"2024-04-22T18:39:58.112092Z","iopub.status.idle":"2024-04-22T18:39:59.342305Z","shell.execute_reply.started":"2024-04-22T18:39:58.112050Z","shell.execute_reply":"2024-04-22T18:39:59.341447Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4fcde669c564918aeae235968f68416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d97e1628a3f45b9a735a715aff40d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cc57b4524d423ba681b274e7ad0893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a721dec20ecf48f3b3d10254a81bb04f"}},"metadata":{}}]},{"cell_type":"code","source":"class MyModel2(nn.Module):\n    def __init__(self, bert_model_name, lstm_hidden_size, lstm_layers, num_classes):\n        super(MyModel2, self).__init__()\n        # BERT Model\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        bert_output_size = self.bert.config.hidden_size\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size=bert_output_size, hidden_size=lstm_hidden_size, \n                            num_layers=lstm_layers, batch_first=True)\n        # Fully connected layers for combined features\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden_size, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Handling text data with BERT\n        with torch.no_grad():\n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_embeddings = bert_output.last_hidden_state\n\n        # Passing BERT embeddings to LSTM\n        lstm_output, _ = self.lstm(bert_embeddings)\n        # Using only the last hidden state of LSTM\n        lstm_output = lstm_output[:, -1, :]\n\n        # Classifier\n        output = self.classifier(lstm_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:02.474987Z","iopub.execute_input":"2024-04-22T18:40:02.475640Z","iopub.status.idle":"2024-04-22T18:40:02.484624Z","shell.execute_reply.started":"2024-04-22T18:40:02.475610Z","shell.execute_reply":"2024-04-22T18:40:02.483699Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Importing Encoder+LSTM Model as Discriminator","metadata":{}},{"cell_type":"code","source":"device = \"cuda\"\nlstm_model = torch.load('/kaggle/input/twitter-data-virality/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:12.156520Z","iopub.execute_input":"2024-04-22T18:40:12.156816Z","iopub.status.idle":"2024-04-22T18:40:12.538395Z","shell.execute_reply.started":"2024-04-22T18:40:12.156792Z","shell.execute_reply":"2024-04-22T18:40:12.537501Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def score_text(text_in):\n    input_id, mask = preprocess_text(text_in)\n    input_id, mask = input_id.to(device), mask.to(device)\n    output = lstm_model(input_id, mask)\n    torch.cuda.empty_cache()\n    \n    return [out[0].item() for out in output]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:16.631909Z","iopub.execute_input":"2024-04-22T18:40:16.632780Z","iopub.status.idle":"2024-04-22T18:40:16.638199Z","shell.execute_reply.started":"2024-04-22T18:40:16.632746Z","shell.execute_reply":"2024-04-22T18:40:16.637160Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Import Processed Twitter data with text and virality","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/twitter-data-virality/dataset_viralscore_3.csv\", low_memory=False)\ndf['UserFavouritesCount'] = pd.to_numeric(df['UserFavouritesCount'], errors='coerce')\ndf = df.dropna(subset=['UserFavouritesCount'])\ndf['mediaCount'] = pd.to_numeric(df['mediaCount'], errors='coerce')\ndf = df.dropna(subset=['mediaCount'])\ndf = df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:24.182913Z","iopub.execute_input":"2024-04-22T18:40:24.183780Z","iopub.status.idle":"2024-04-22T18:40:25.671983Z","shell.execute_reply.started":"2024-04-22T18:40:24.183746Z","shell.execute_reply":"2024-04-22T18:40:25.670986Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_content = pd.DataFrame(df['content'])\ndf_content = df_content[df_content['content'].apply(lambda x: len(x.split()) >= 10)]\ndf_content['cleaned'] = df_content['content'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:27.748016Z","iopub.execute_input":"2024-04-22T18:40:27.748792Z","iopub.status.idle":"2024-04-22T18:40:27.982755Z","shell.execute_reply.started":"2024-04-22T18:40:27.748756Z","shell.execute_reply":"2024-04-22T18:40:27.981726Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"mask = df_content['cleaned'].str.contains(r'\\[URL\\]')\ndf_filtered = df_content[~mask]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:31.468213Z","iopub.execute_input":"2024-04-22T18:40:31.468575Z","iopub.status.idle":"2024-04-22T18:40:31.500786Z","shell.execute_reply.started":"2024-04-22T18:40:31.468548Z","shell.execute_reply":"2024-04-22T18:40:31.499621Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Create GPT2 Generator Model, Define Functions to generate new text given input","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n    model.resize_token_embeddings(len(tokenizer)) \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:34.008825Z","iopub.execute_input":"2024-04-22T18:40:34.009202Z","iopub.status.idle":"2024-04-22T18:40:38.723482Z","shell.execute_reply.started":"2024-04-22T18:40:34.009174Z","shell.execute_reply":"2024-04-22T18:40:38.722244Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52843f781bb4d07a4c7c0ced30ef4f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb8b4a14623d4d90aedd177d35856bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd022c8f12ec40949063b6780f2f96ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2843e0359db44c39b8a4f5cd64665e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92870bbca99348f7b4dae2c108289ea3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d07f820408d43fb9e0bdd5c0353ce60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb79294a64c406d9689afdd30d1ad33"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def alter_tweet(model, tokenizer, tweet, prompt_text):\n    \n    inputs = tokenizer(prompt_text+tweet+\": \", padding=True, return_tensors='pt')\n    inputs.to(device)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    prompt_length = input_ids.shape[1]\n\n    # Generate new text\n\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=prompt_length*2,  # Add a reasonable number of tokens, adjust as needed\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n        temperature=0.7,\n        top_k=55,\n        top_p=0.95,\n        no_repeat_ngram_size=2,\n        num_return_sequences=1\n    )\n\n    # Decode generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    original_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n    # Remove the original part from the generated text if present\n    altered_text = generated_text[len(original_text):].strip()\n    torch.cuda.empty_cache()\n    return altered_text if altered_text else generated_text  # Fallback to generated if slicing fails","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:41.051222Z","iopub.execute_input":"2024-04-22T18:40:41.051587Z","iopub.status.idle":"2024-04-22T18:40:41.059683Z","shell.execute_reply.started":"2024-04-22T18:40:41.051563Z","shell.execute_reply":"2024-04-22T18:40:41.058631Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"base_model = GPT2LMHeadModel.from_pretrained('gpt2')\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n    base_model.resize_token_embeddings(len(tokenizer)) \nbase_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:40:44.384762Z","iopub.execute_input":"2024-04-22T18:40:44.385138Z","iopub.status.idle":"2024-04-22T18:40:45.070351Z","shell.execute_reply.started":"2024-04-22T18:40:44.385109Z","shell.execute_reply":"2024-04-22T18:40:45.069393Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Test Model Text Generation","metadata":{}},{"cell_type":"code","source":"model.eval()\nprompt_text = \"Rewrite this phrase, without changing the meaning: \"\nsample_tweet = df_filtered['cleaned'].sample(n=1).iloc[0].replace(\"\\n\",\"\")\n#sample_tweet = \"CS6120 by Raj is the best class ever! Highly Recommend!\"\nprint(\"Original Tweet:\", sample_tweet)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:41:43.628714Z","iopub.execute_input":"2024-04-22T18:41:43.629121Z","iopub.status.idle":"2024-04-22T18:41:43.637354Z","shell.execute_reply.started":"2024-04-22T18:41:43.629070Z","shell.execute_reply":"2024-04-22T18:41:43.636307Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Original Tweet: Plan to travel over the mountains to [MENTION] for the #AppleCup2022? Be planning ahead, leave for Pullman earlier on Saturday if possible and be patient. Snow is expected. Make sure to follow traction requirements and drive for conditions.\n","output_type":"stream"}]},{"cell_type":"code","source":"altered_tweet = alter_tweet(model, tokenizer, sample_tweet, prompt_text)\n#altered2_tweet = alter_tweet(model2, tokenizer, sample_tweet, prompt_text)\nuntrained_tweet = alter_tweet(base_model, tokenizer, sample_tweet, prompt_text)\nprint(\"Original Tweet:\", sample_tweet)\nprint(\"Altered Tweet:\", clean_text(altered_tweet).replace(\"\\n\",\"\"))\n#print(\"Altered2 Tweet:\", clean_text(altered2_tweet).replace(\"\\n\",\"\"))\nprint(\"Untrained Tweet:\", clean_text(untrained_tweet).replace(\"\\n\",\"\"))\nprint(f\"Original Tweet Score: {score_text(sample_tweet)[0]:0.4f}\")\nprint(f'Altered Tweet Score: {score_text(altered_tweet)[0]:0.4f}')\n#print(f'Altered2 Tweet Score: {score_text(altered2_tweet)[0]:0.4f}')\nprint(f'Untrained Tweet Score: {score_text(untrained_tweet)[0]:0.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:56:07.907548Z","iopub.execute_input":"2024-04-22T18:56:07.907984Z","iopub.status.idle":"2024-04-22T18:56:09.340235Z","shell.execute_reply.started":"2024-04-22T18:56:07.907948Z","shell.execute_reply":"2024-04-22T18:56:09.339341Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Original Tweet: Plan to travel over the mountains to [MENTION] for the #AppleCup2022? Be planning ahead, leave for Pullman earlier on Saturday if possible and be patient. Snow is expected. Make sure to follow traction requirements and drive for conditions.\nAltered Tweet: I'm going to be a little late. I'm not going. The weather is going bad. It's going good. We're going down the mountain.I am a man. A man who is a woman. And a girl. A man is the man I am. He is my\nUntrained Tweet: I'm going to be driving for a few hours on the road, so I'm not going anywhere. I'll be in the car for about an hour. If you're not sure what to do, just drive. It's a good idea to drive in a straight line, but I don't think it\nOriginal Tweet Score: -0.4329\nAltered Tweet Score: 0.2006\nUntrained Tweet Score: -0.8736\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training and Verification Loop. Calculate Loss based on Similarity and Virality","metadata":{}},{"cell_type":"code","source":"def calculate_log_prob(model, tokenizer, inputs, next_token):\n    outputs = model(inputs.to(device))\n    logits = outputs.logits[:, -1, :]  # Get the logits for the last token position\n    log_probs = log_softmax(logits, dim=-1)\n    next_token_log_prob = log_probs.gather(1, next_token.to(device).unsqueeze(-1)).squeeze(-1)\n    return next_token_log_prob","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:24:32.711929Z","iopub.execute_input":"2024-04-22T16:24:32.712616Z","iopub.status.idle":"2024-04-22T16:24:32.718343Z","shell.execute_reply.started":"2024-04-22T16:24:32.712578Z","shell.execute_reply":"2024-04-22T16:24:32.717357Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T19:31:39.371435Z","iopub.execute_input":"2024-04-19T19:31:39.371811Z","iopub.status.idle":"2024-04-19T19:31:39.385182Z","shell.execute_reply.started":"2024-04-19T19:31:39.371781Z","shell.execute_reply":"2024-04-19T19:31:39.384198Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef calculate_similarity(text1, text2):\n    embeddings1 = similarity_model.encode(text1, convert_to_tensor=True, show_progress_bar=False)\n    embeddings2 = similarity_model.encode(text2, convert_to_tensor=True, show_progress_bar=False)\n    cosine_sim = util.pytorch_cos_sim(embeddings1, embeddings2)\n    return cosine_sim.item()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:24:35.447261Z","iopub.execute_input":"2024-04-22T16:24:35.447627Z","iopub.status.idle":"2024-04-22T16:24:44.280182Z","shell.execute_reply.started":"2024-04-22T16:24:35.447598Z","shell.execute_reply":"2024-04-22T16:24:44.279236Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f443085d1724bc2a69f6d10a6ace92e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6e747be3d741fc8da10e47e5d5f340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3c69dde9944de6a975b026a6788037"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8cedd9f4850410bb8017347e778dfc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcddd30af4944d35bdeaa9e2c2a169c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f893997fa2fb43c5bceccf4f4f425f17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb1aa11e2ecb45a08e0467eacec44314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9141f2932c34c7ba58fd37e754fa236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c744e76d5c34e6eb58253ad7e9e5af8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d946a74dcd429680c08372e88469fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2bee258b71a4146829edc30f8afb2cf"}},"metadata":{}}]},{"cell_type":"code","source":"epochs = 50\ndevice = \"cuda\"\nmodel.to(device)\nmodel.train()\nnum_tweets_to_process = 10\nprompt_text = prompt_text\n\nalpha = 1\nbeta = 0\n\nmax_length = 128\nfor epoch in range(epochs):\n    shuffled_tweets = df_filtered['cleaned'].sample(frac=1).iloc[:num_tweets_to_process]\n    total_reward = 0\n    for tweet in tqdm(shuffled_tweets, desc=f\"Epoch {epoch + 1} Progress\"):\n        origin_score = torch.sigmoid(torch.tensor(score_text(tweet)[0]))\n        altered_text = alter_tweet(model, tokenizer, tweet, prompt_text)\n        content_score = torch.sigmoid(torch.tensor(score_text(altered_text)[0]))\n        similarity_score = calculate_similarity(tweet, altered_text)\n        \n        #Either Base the reward on Similarity-Virality Combination or Score Improvement\n        #reward = alpha*content_score + beta*((similarity_score - 0.5)*2)\n        reward = content_score - origin_score\n\n        inputs = tokenizer(altered_text, return_tensors='pt')\n        input_ids = inputs['input_ids'].to(device)\n        attention_mask = inputs['attention_mask'].to(device)\n\n        model.zero_grad()\n        total_log_prob = torch.tensor(0.0, device=device).unsqueeze(0) \n        \n        #Calculate the Log Probability of the given altered text sequence\n        for i in range(1, input_ids.size(1)):\n            current_input = input_ids[:, :i]\n            next_token = input_ids[:, i]\n            log_prob = calculate_log_prob(model, tokenizer, current_input, next_token)\n            total_log_prob += log_prob       \n        \n        total_reward += reward\n\n        loss = -total_log_prob * reward\n        if loss.item() != 0:\n            loss.backward()\n            optimizer.step()\n        optimizer.zero_grad()\n        torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    print(f\"Epoch {epoch + 1}, Total Reward: {total_reward}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-19T19:31:43.939638Z","iopub.execute_input":"2024-04-19T19:31:43.940288Z","iopub.status.idle":"2024-04-19T19:39:34.748570Z","shell.execute_reply.started":"2024-04-19T19:31:43.940254Z","shell.execute_reply":"2024-04-19T19:39:34.747319Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Epoch 1 Progress: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Total Reward: -0.09869101643562317\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Progress: 100%|██████████| 10/10 [00:16<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Total Reward: 0.6804487705230713\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Progress: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Total Reward: -0.20026174187660217\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Total Reward: -0.20271167159080505\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Progress: 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Total Reward: 0.26110512018203735\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Total Reward: 0.14610505104064941\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 Progress: 100%|██████████| 10/10 [00:14<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Total Reward: 0.5619194507598877\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Total Reward: 0.639773964881897\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 Progress: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Total Reward: 0.33377277851104736\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 Progress: 100%|██████████| 10/10 [00:17<00:00,  1.78s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Total Reward: 1.0590357780456543\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11, Total Reward: 1.0587314367294312\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 Progress: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12, Total Reward: 0.2766219973564148\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 Progress: 100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13, Total Reward: 0.0862874686717987\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14 Progress: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14, Total Reward: 0.3303617238998413\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15 Progress: 100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15, Total Reward: 1.0817337036132812\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16 Progress: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16, Total Reward: 0.051283687353134155\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17 Progress: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17, Total Reward: 0.2521398067474365\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18, Total Reward: 0.32885897159576416\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19 Progress: 100%|██████████| 10/10 [00:16<00:00,  1.67s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19, Total Reward: 0.5815116763114929\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20 Progress: 100%|██████████| 10/10 [00:17<00:00,  1.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Total Reward: 0.3492371439933777\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21, Total Reward: 0.3700333535671234\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22 Progress: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22, Total Reward: 0.03997910022735596\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23 Progress: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23, Total Reward: 0.6178473830223083\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24 Progress: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24, Total Reward: 0.7420645952224731\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25 Progress: 100%|██████████| 10/10 [00:19<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25, Total Reward: 0.49961385130882263\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26 Progress:  20%|██        | 2/10 [00:03<00:14,  1.79s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtotal_log_prob \u001b[38;5;241m*\u001b[39m reward\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Save or Load Previous Models as Necessary","metadata":{}},{"cell_type":"code","source":"model = torch.load('/kaggle/input/twitter-data-virality/generator_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:41:04.440233Z","iopub.execute_input":"2024-04-22T18:41:04.440569Z","iopub.status.idle":"2024-04-22T18:41:10.669391Z","shell.execute_reply.started":"2024-04-22T18:41:04.440545Z","shell.execute_reply":"2024-04-22T18:41:10.668342Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model2 = torch.load('/kaggle/input/twitter-data-virality/generator_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T01:15:52.336565Z","iopub.execute_input":"2024-04-20T01:15:52.337234Z","iopub.status.idle":"2024-04-20T01:15:57.069111Z","shell.execute_reply.started":"2024-04-20T01:15:52.337166Z","shell.execute_reply":"2024-04-20T01:15:57.068229Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/generator_model_2.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T19:54:58.740577Z","iopub.execute_input":"2024-04-19T19:54:58.741253Z","iopub.status.idle":"2024-04-19T19:54:59.501172Z","shell.execute_reply.started":"2024-04-19T19:54:58.741221Z","shell.execute_reply":"2024-04-19T19:54:59.500255Z"},"trusted":true},"execution_count":48,"outputs":[]}]}