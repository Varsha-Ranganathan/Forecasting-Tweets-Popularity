{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8169760,"sourceType":"datasetVersion","datasetId":4759473}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TF-IDF and Encoder-LSTM, LIME Aggregator","metadata":{}},{"cell_type":"markdown","source":"### Install Necessary Libraries","metadata":{}},{"cell_type":"code","source":"#pip install torchtext\n#!pip install transformers\n#!pip install torchsummary ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.utils import simple_preprocess\nimport torch\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, WeightedRandomSampler\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import Vocab\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n%matplotlib inline\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom torch.utils.data.dataset import random_split\nfrom scipy.sparse import csr_matrix\nfrom transformers import BertModel\nimport shap\nfrom lime.lime_text import LimeTextExplainer\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:05:17.681799Z","iopub.execute_input":"2024-04-22T23:05:17.682997Z","iopub.status.idle":"2024-04-22T23:05:46.460209Z","shell.execute_reply.started":"2024-04-22T23:05:17.682937Z","shell.execute_reply":"2024-04-22T23:05:46.459367Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"2024-04-22 23:05:34.640065: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 23:05:34.640164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 23:05:34.774962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Twitter Dataset, Clean and Select Features for Models","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/twitter-data-virality/dataset_viralscore_2.csv\", low_memory=False)\ndf['UserFavouritesCount'] = pd.to_numeric(df['UserFavouritesCount'], errors='coerce')\ndf = df.dropna(subset=['UserFavouritesCount'])\ndf['mediaCount'] = pd.to_numeric(df['mediaCount'], errors='coerce')\ndf = df.dropna(subset=['mediaCount'])\ndf = df.sample(frac=1).reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:21.087641Z","iopub.execute_input":"2024-04-22T23:13:21.088462Z","iopub.status.idle":"2024-04-22T23:13:22.851250Z","shell.execute_reply.started":"2024-04-22T23:13:21.088429Z","shell.execute_reply":"2024-04-22T23:13:22.850313Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                     url  \\\n0      https://twitter.com/pannchoa/status/1589407272...   \n1      https://twitter.com/ovokkj/status/154447107448...   \n2      https://twitter.com/milkstrology/status/156766...   \n3      https://twitter.com/TxQueenTee/status/15017093...   \n4      https://twitter.com/minarchist9/status/1511856...   \n...                                                  ...   \n43787  https://twitter.com/makeitslowerr/status/15256...   \n43788  https://twitter.com/fuckinbiscuit/status/15676...   \n43789  https://twitter.com/m_sarr/status/157418688694...   \n43790  https://twitter.com/LukeThisisnot/status/15491...   \n43791  https://twitter.com/minapaws/status/1498810260...   \n\n                            date  \\\n0      2022-11-06 23:59:58+00:00   \n1      2022-07-05 23:59:33+00:00   \n2      2022-09-07 23:59:41+00:00   \n3      2022-03-09 23:59:41+00:00   \n4      2022-04-06 23:59:46+00:00   \n...                          ...   \n43787  2022-05-14 23:59:40+00:00   \n43788  2022-09-07 23:59:57+00:00   \n43789  2022-09-25 23:59:35+00:00   \n43790  2022-07-18 23:59:50+00:00   \n43791  2022-03-01 23:59:47+00:00   \n\n                                                 content  likeCount  \\\n0      Red Velvet releases The ReVe Festival 2022 tea...      889.0   \n1                         diazin https://t.co/XVx1NQOiwT       11.0   \n2      venus in virgo squaring mars in gemini reminds...      667.0   \n3      now that page is attacking me and y’all wanna ...       38.0   \n4                        @folding_laundry not a bad take        1.0   \n...                                                  ...        ...   \n43787  what they’re about to do is literally life alt...       16.0   \n43788  @emersonRpage idk what’s up with my luck fr bu...        0.0   \n43789  How they expect Melissa not to date?  #BlackWi...        2.0   \n43790  Seems like a fitting end to this Monday. https...       16.0   \n43791  - Okay I won’t do the pose\\n“No,no. Do it! I’l...      174.0   \n\n       replyCount  retweetCount     viewCount  quoteCount  \\\n0            16.0           178  58702.620000          51   \n1             5.0             0   1202.246025           1   \n2             6.0            45  26079.540000          24   \n3            11.0             1   3240.231232           0   \n4             0.0             0    191.910011           0   \n...           ...           ...           ...         ...   \n43787         1.0             0   1902.447461           0   \n43788         0.0             0    210.006559           0   \n43789         0.0             0    302.782800           0   \n43790         4.0             0   1902.447461           0   \n43791         2.0            39  25667.113520           4   \n\n               sourceLabel  links  ...  \\\n0          Twitter Web App      1  ...   \n1      Twitter for Android      0  ...   \n2          Twitter Web App      0  ...   \n3       Twitter for iPhone      0  ...   \n4          Twitter Web App      0  ...   \n...                    ...    ...  ...   \n43787   Twitter for iPhone      0  ...   \n43788   Twitter for iPhone      0  ...   \n43789   Twitter for iPhone      0  ...   \n43790  Twitter for Android      0  ...   \n43791     Twitter for iPad      1  ...   \n\n                                               tokenized  \\\n0      ['Red', 'Velvet', 'releases', 'The', 'ReVe', '...   \n1                                             ['diazin']   \n2      ['venus', 'virgo', 'squaring', 'mars', 'gemini...   \n3      ['page', 'attacking', 'wanna', 'know', 'whats'...   \n4                     ['folding_laundry', 'bad', 'take']   \n...                                                  ...   \n43787  ['literally', 'life', 'altering', 'magic', 'fe...   \n43788  ['emersonRpage', 'idk', 'luck', 'fr', 'somehow...   \n43789  ['How', 'expect', 'Melissa', 'date', 'BlackWid...   \n43790      ['Seems', 'like', 'fitting', 'end', 'Monday']   \n43791  ['Okay', 'I', 'pose', 'No', 'Do', 'I', 'close'...   \n\n                                            cleaned_text  likeCount_log  \\\n0      Red Velvet releases The ReVe Festival teaser i...       0.509429   \n1                                                 diazin       0.186400   \n2      venus virgo squaring mars gemini reminds hyper...       0.487906   \n3      page attacking wanna know whats funny screensh...       0.274814   \n4                               folding_laundry bad take       0.051995   \n...                                                  ...            ...   \n43787  literally life altering magic feel ready terri...       0.212528   \n43788  emersonRpage idk luck fr somehow every health ...       0.000000   \n43789          How expect Melissa date BlackWidowMurders       0.082410   \n43790                      Seems like fitting end Monday       0.212528   \n43791    Okay I pose No Do I close eyes ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅠㅠㅠㅠ       0.387426   \n\n      replyCount_log viewCount_log  totalpop userpopularity totalScore  \\\n0           0.298540      0.544019  143505.0           High   1.351988   \n1           0.188800      0.174161    1003.0            Low   0.549361   \n2           0.205043      0.466830  390932.0           High   1.159779   \n3           0.261838      0.268439    5228.0            Low   0.805091   \n4           0.000000      0.000000     406.0            Low   0.051995   \n...              ...           ...       ...            ...        ...   \n43787       0.073038      0.217797    1870.0            Low   0.503362   \n43788       0.000000      0.008531     558.0            Low   0.008531   \n43789       0.000000      0.043203    2736.0            Low   0.125613   \n43790       0.169589      0.217797    1936.0            Low   0.599913   \n43791       0.115762      0.465314    1586.0            Low   0.968502   \n\n      ViralGroup ViralGroupAdjusted  \n0           High               High  \n1            Low               High  \n2           High               High  \n3           High               High  \n4            Low                Low  \n...          ...                ...  \n43787        Low               High  \n43788        Low                Low  \n43789        Low                Low  \n43790        Low               High  \n43791       High               High  \n\n[43792 rows x 40 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>date</th>\n      <th>content</th>\n      <th>likeCount</th>\n      <th>replyCount</th>\n      <th>retweetCount</th>\n      <th>viewCount</th>\n      <th>quoteCount</th>\n      <th>sourceLabel</th>\n      <th>links</th>\n      <th>...</th>\n      <th>tokenized</th>\n      <th>cleaned_text</th>\n      <th>likeCount_log</th>\n      <th>replyCount_log</th>\n      <th>viewCount_log</th>\n      <th>totalpop</th>\n      <th>userpopularity</th>\n      <th>totalScore</th>\n      <th>ViralGroup</th>\n      <th>ViralGroupAdjusted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://twitter.com/pannchoa/status/1589407272...</td>\n      <td>2022-11-06 23:59:58+00:00</td>\n      <td>Red Velvet releases The ReVe Festival 2022 tea...</td>\n      <td>889.0</td>\n      <td>16.0</td>\n      <td>178</td>\n      <td>58702.620000</td>\n      <td>51</td>\n      <td>Twitter Web App</td>\n      <td>1</td>\n      <td>...</td>\n      <td>['Red', 'Velvet', 'releases', 'The', 'ReVe', '...</td>\n      <td>Red Velvet releases The ReVe Festival teaser i...</td>\n      <td>0.509429</td>\n      <td>0.298540</td>\n      <td>0.544019</td>\n      <td>143505.0</td>\n      <td>High</td>\n      <td>1.351988</td>\n      <td>High</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://twitter.com/ovokkj/status/154447107448...</td>\n      <td>2022-07-05 23:59:33+00:00</td>\n      <td>diazin https://t.co/XVx1NQOiwT</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>1202.246025</td>\n      <td>1</td>\n      <td>Twitter for Android</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['diazin']</td>\n      <td>diazin</td>\n      <td>0.186400</td>\n      <td>0.188800</td>\n      <td>0.174161</td>\n      <td>1003.0</td>\n      <td>Low</td>\n      <td>0.549361</td>\n      <td>Low</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://twitter.com/milkstrology/status/156766...</td>\n      <td>2022-09-07 23:59:41+00:00</td>\n      <td>venus in virgo squaring mars in gemini reminds...</td>\n      <td>667.0</td>\n      <td>6.0</td>\n      <td>45</td>\n      <td>26079.540000</td>\n      <td>24</td>\n      <td>Twitter Web App</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['venus', 'virgo', 'squaring', 'mars', 'gemini...</td>\n      <td>venus virgo squaring mars gemini reminds hyper...</td>\n      <td>0.487906</td>\n      <td>0.205043</td>\n      <td>0.466830</td>\n      <td>390932.0</td>\n      <td>High</td>\n      <td>1.159779</td>\n      <td>High</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://twitter.com/TxQueenTee/status/15017093...</td>\n      <td>2022-03-09 23:59:41+00:00</td>\n      <td>now that page is attacking me and y’all wanna ...</td>\n      <td>38.0</td>\n      <td>11.0</td>\n      <td>1</td>\n      <td>3240.231232</td>\n      <td>0</td>\n      <td>Twitter for iPhone</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['page', 'attacking', 'wanna', 'know', 'whats'...</td>\n      <td>page attacking wanna know whats funny screensh...</td>\n      <td>0.274814</td>\n      <td>0.261838</td>\n      <td>0.268439</td>\n      <td>5228.0</td>\n      <td>Low</td>\n      <td>0.805091</td>\n      <td>High</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://twitter.com/minarchist9/status/1511856...</td>\n      <td>2022-04-06 23:59:46+00:00</td>\n      <td>@folding_laundry not a bad take</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>191.910011</td>\n      <td>0</td>\n      <td>Twitter Web App</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['folding_laundry', 'bad', 'take']</td>\n      <td>folding_laundry bad take</td>\n      <td>0.051995</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>406.0</td>\n      <td>Low</td>\n      <td>0.051995</td>\n      <td>Low</td>\n      <td>Low</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43787</th>\n      <td>https://twitter.com/makeitslowerr/status/15256...</td>\n      <td>2022-05-14 23:59:40+00:00</td>\n      <td>what they’re about to do is literally life alt...</td>\n      <td>16.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1902.447461</td>\n      <td>0</td>\n      <td>Twitter for iPhone</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['literally', 'life', 'altering', 'magic', 'fe...</td>\n      <td>literally life altering magic feel ready terri...</td>\n      <td>0.212528</td>\n      <td>0.073038</td>\n      <td>0.217797</td>\n      <td>1870.0</td>\n      <td>Low</td>\n      <td>0.503362</td>\n      <td>Low</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>43788</th>\n      <td>https://twitter.com/fuckinbiscuit/status/15676...</td>\n      <td>2022-09-07 23:59:57+00:00</td>\n      <td>@emersonRpage idk what’s up with my luck fr bu...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>210.006559</td>\n      <td>0</td>\n      <td>Twitter for iPhone</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['emersonRpage', 'idk', 'luck', 'fr', 'somehow...</td>\n      <td>emersonRpage idk luck fr somehow every health ...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008531</td>\n      <td>558.0</td>\n      <td>Low</td>\n      <td>0.008531</td>\n      <td>Low</td>\n      <td>Low</td>\n    </tr>\n    <tr>\n      <th>43789</th>\n      <td>https://twitter.com/m_sarr/status/157418688694...</td>\n      <td>2022-09-25 23:59:35+00:00</td>\n      <td>How they expect Melissa not to date?  #BlackWi...</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>302.782800</td>\n      <td>0</td>\n      <td>Twitter for iPhone</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['How', 'expect', 'Melissa', 'date', 'BlackWid...</td>\n      <td>How expect Melissa date BlackWidowMurders</td>\n      <td>0.082410</td>\n      <td>0.000000</td>\n      <td>0.043203</td>\n      <td>2736.0</td>\n      <td>Low</td>\n      <td>0.125613</td>\n      <td>Low</td>\n      <td>Low</td>\n    </tr>\n    <tr>\n      <th>43790</th>\n      <td>https://twitter.com/LukeThisisnot/status/15491...</td>\n      <td>2022-07-18 23:59:50+00:00</td>\n      <td>Seems like a fitting end to this Monday. https...</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>1902.447461</td>\n      <td>0</td>\n      <td>Twitter for Android</td>\n      <td>0</td>\n      <td>...</td>\n      <td>['Seems', 'like', 'fitting', 'end', 'Monday']</td>\n      <td>Seems like fitting end Monday</td>\n      <td>0.212528</td>\n      <td>0.169589</td>\n      <td>0.217797</td>\n      <td>1936.0</td>\n      <td>Low</td>\n      <td>0.599913</td>\n      <td>Low</td>\n      <td>High</td>\n    </tr>\n    <tr>\n      <th>43791</th>\n      <td>https://twitter.com/minapaws/status/1498810260...</td>\n      <td>2022-03-01 23:59:47+00:00</td>\n      <td>- Okay I won’t do the pose\\n“No,no. Do it! I’l...</td>\n      <td>174.0</td>\n      <td>2.0</td>\n      <td>39</td>\n      <td>25667.113520</td>\n      <td>4</td>\n      <td>Twitter for iPad</td>\n      <td>1</td>\n      <td>...</td>\n      <td>['Okay', 'I', 'pose', 'No', 'Do', 'I', 'close'...</td>\n      <td>Okay I pose No Do I close eyes ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅠㅠㅠㅠ</td>\n      <td>0.387426</td>\n      <td>0.115762</td>\n      <td>0.465314</td>\n      <td>1586.0</td>\n      <td>Low</td>\n      <td>0.968502</td>\n      <td>High</td>\n      <td>High</td>\n    </tr>\n  </tbody>\n</table>\n<p>43792 rows × 40 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.groupby('ViralGroupAdjusted').size()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:28.367980Z","iopub.execute_input":"2024-04-22T23:13:28.368341Z","iopub.status.idle":"2024-04-22T23:13:28.384197Z","shell.execute_reply.started":"2024-04-22T23:13:28.368315Z","shell.execute_reply":"2024-04-22T23:13:28.383259Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"ViralGroupAdjusted\nHigh    21896\nLow     21896\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"select_columns = ['links','media','quotedTweet','mentionedUsers','UserFavouritesCount','followersCount','friendsCount','verified','protected','mediaCount','predicted_sentiment']\n#select_columns = ['links','media','quotedTweet','mentionedUsers','UserFavouritesCount','friendsCount','protected','mediaCount']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:30.892484Z","iopub.execute_input":"2024-04-22T23:13:30.893370Z","iopub.status.idle":"2024-04-22T23:13:30.897698Z","shell.execute_reply.started":"2024-04-22T23:13:30.893338Z","shell.execute_reply":"2024-04-22T23:13:30.896746Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_data = df[select_columns].copy()\ndf_data","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:32.690076Z","iopub.execute_input":"2024-04-22T23:13:32.690885Z","iopub.status.idle":"2024-04-22T23:13:32.710480Z","shell.execute_reply.started":"2024-04-22T23:13:32.690853Z","shell.execute_reply":"2024-04-22T23:13:32.709594Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       links  media  quotedTweet  mentionedUsers  UserFavouritesCount  \\\n0          1      1            0               0                 5641   \n1          0      1            0               0                18108   \n2          0      0            0               0                71117   \n3          0      0            0               0                82866   \n4          0      0            0               1                 1893   \n...      ...    ...          ...             ...                  ...   \n43787      0      1            0               0               123217   \n43788      0      0            0               0                18710   \n43789      0      1            0               0                88313   \n43790      0      1            0               0                17084   \n43791      1      0            0               0                28077   \n\n       followersCount  friendsCount  verified  protected  mediaCount  \\\n0            143474.0            31         0          0     14731.0   \n1               293.0           710         0          0       259.0   \n2            389770.0          1162         1          0      8760.0   \n3              2854.0          2374         0          0     11232.0   \n4                79.0           327         0          0        90.0   \n...               ...           ...       ...        ...         ...   \n43787          1141.0           729         0          0      4673.0   \n43788           292.0           266         0          0       177.0   \n43789          1630.0          1106         0          0     21350.0   \n43790          1293.0           643         0          0      2092.0   \n43791          1398.0           188         0          0      4751.0   \n\n      predicted_sentiment  \n0                 Neutral  \n1                 Neutral  \n2                Negative  \n3                Positive  \n4                Negative  \n...                   ...  \n43787            Positive  \n43788            Negative  \n43789             Neutral  \n43790             Neutral  \n43791            Negative  \n\n[43792 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>links</th>\n      <th>media</th>\n      <th>quotedTweet</th>\n      <th>mentionedUsers</th>\n      <th>UserFavouritesCount</th>\n      <th>followersCount</th>\n      <th>friendsCount</th>\n      <th>verified</th>\n      <th>protected</th>\n      <th>mediaCount</th>\n      <th>predicted_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5641</td>\n      <td>143474.0</td>\n      <td>31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14731.0</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18108</td>\n      <td>293.0</td>\n      <td>710</td>\n      <td>0</td>\n      <td>0</td>\n      <td>259.0</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>71117</td>\n      <td>389770.0</td>\n      <td>1162</td>\n      <td>1</td>\n      <td>0</td>\n      <td>8760.0</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82866</td>\n      <td>2854.0</td>\n      <td>2374</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11232.0</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1893</td>\n      <td>79.0</td>\n      <td>327</td>\n      <td>0</td>\n      <td>0</td>\n      <td>90.0</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43787</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>123217</td>\n      <td>1141.0</td>\n      <td>729</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4673.0</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>43788</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18710</td>\n      <td>292.0</td>\n      <td>266</td>\n      <td>0</td>\n      <td>0</td>\n      <td>177.0</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>43789</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88313</td>\n      <td>1630.0</td>\n      <td>1106</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21350.0</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>43790</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17084</td>\n      <td>1293.0</td>\n      <td>643</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2092.0</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>43791</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>28077</td>\n      <td>1398.0</td>\n      <td>188</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4751.0</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>43792 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sentiment_mapping = {'Negative': -1, 'Neutral': 0, 'Positive': 1}\ndf_data['predicted_sentiment'] = df_data['predicted_sentiment'].map(sentiment_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:35.615587Z","iopub.execute_input":"2024-04-22T23:13:35.616392Z","iopub.status.idle":"2024-04-22T23:13:35.626666Z","shell.execute_reply.started":"2024-04-22T23:13:35.616358Z","shell.execute_reply":"2024-04-22T23:13:35.625711Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_data","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:37.469920Z","iopub.execute_input":"2024-04-22T23:13:37.470725Z","iopub.status.idle":"2024-04-22T23:13:37.491315Z","shell.execute_reply.started":"2024-04-22T23:13:37.470687Z","shell.execute_reply":"2024-04-22T23:13:37.490334Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       links  media  quotedTweet  mentionedUsers  UserFavouritesCount  \\\n0          1      1            0               0                 5641   \n1          0      1            0               0                18108   \n2          0      0            0               0                71117   \n3          0      0            0               0                82866   \n4          0      0            0               1                 1893   \n...      ...    ...          ...             ...                  ...   \n43787      0      1            0               0               123217   \n43788      0      0            0               0                18710   \n43789      0      1            0               0                88313   \n43790      0      1            0               0                17084   \n43791      1      0            0               0                28077   \n\n       followersCount  friendsCount  verified  protected  mediaCount  \\\n0            143474.0            31         0          0     14731.0   \n1               293.0           710         0          0       259.0   \n2            389770.0          1162         1          0      8760.0   \n3              2854.0          2374         0          0     11232.0   \n4                79.0           327         0          0        90.0   \n...               ...           ...       ...        ...         ...   \n43787          1141.0           729         0          0      4673.0   \n43788           292.0           266         0          0       177.0   \n43789          1630.0          1106         0          0     21350.0   \n43790          1293.0           643         0          0      2092.0   \n43791          1398.0           188         0          0      4751.0   \n\n       predicted_sentiment  \n0                        0  \n1                        0  \n2                       -1  \n3                        1  \n4                       -1  \n...                    ...  \n43787                    1  \n43788                   -1  \n43789                    0  \n43790                    0  \n43791                   -1  \n\n[43792 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>links</th>\n      <th>media</th>\n      <th>quotedTweet</th>\n      <th>mentionedUsers</th>\n      <th>UserFavouritesCount</th>\n      <th>followersCount</th>\n      <th>friendsCount</th>\n      <th>verified</th>\n      <th>protected</th>\n      <th>mediaCount</th>\n      <th>predicted_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5641</td>\n      <td>143474.0</td>\n      <td>31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14731.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18108</td>\n      <td>293.0</td>\n      <td>710</td>\n      <td>0</td>\n      <td>0</td>\n      <td>259.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>71117</td>\n      <td>389770.0</td>\n      <td>1162</td>\n      <td>1</td>\n      <td>0</td>\n      <td>8760.0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82866</td>\n      <td>2854.0</td>\n      <td>2374</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11232.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1893</td>\n      <td>79.0</td>\n      <td>327</td>\n      <td>0</td>\n      <td>0</td>\n      <td>90.0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43787</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>123217</td>\n      <td>1141.0</td>\n      <td>729</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4673.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43788</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18710</td>\n      <td>292.0</td>\n      <td>266</td>\n      <td>0</td>\n      <td>0</td>\n      <td>177.0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>43789</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>88313</td>\n      <td>1630.0</td>\n      <td>1106</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21350.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43790</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17084</td>\n      <td>1293.0</td>\n      <td>643</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2092.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43791</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>28077</td>\n      <td>1398.0</td>\n      <td>188</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4751.0</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>43792 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df_data.dtypes)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:40.274273Z","iopub.execute_input":"2024-04-22T23:13:40.274662Z","iopub.status.idle":"2024-04-22T23:13:40.280935Z","shell.execute_reply.started":"2024-04-22T23:13:40.274633Z","shell.execute_reply":"2024-04-22T23:13:40.279912Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"links                    int64\nmedia                    int64\nquotedTweet              int64\nmentionedUsers           int64\nUserFavouritesCount      int64\nfollowersCount         float64\nfriendsCount             int64\nverified                 int64\nprotected                int64\nmediaCount             float64\npredicted_sentiment      int64\ndtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"for col in df_data.columns:\n    if df_data[col].dtype == np.object_:\n        # Attempt to convert to float; handle exceptions as needed\n        df_data[col] = df_data[col].astype(float)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:42.138527Z","iopub.execute_input":"2024-04-22T23:13:42.138874Z","iopub.status.idle":"2024-04-22T23:13:42.144205Z","shell.execute_reply.started":"2024-04-22T23:13:42.138848Z","shell.execute_reply":"2024-04-22T23:13:42.143181Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_data), columns=df_data.columns)\ndf_scaled","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:44.953908Z","iopub.execute_input":"2024-04-22T23:13:44.954725Z","iopub.status.idle":"2024-04-22T23:13:44.984243Z","shell.execute_reply.started":"2024-04-22T23:13:44.954695Z","shell.execute_reply":"2024-04-22T23:13:44.983299Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"          links     media  quotedTweet  mentionedUsers  UserFavouritesCount  \\\n0      2.985327  1.247817    -0.458066       -0.419791            -0.537304   \n1     -0.334972  1.247817    -0.458066       -0.419791            -0.406681   \n2     -0.334972 -0.801399    -0.458066       -0.419791             0.148718   \n3     -0.334972 -0.801399    -0.458066       -0.419791             0.271818   \n4     -0.334972 -0.801399    -0.458066        2.382140            -0.576573   \n...         ...       ...          ...             ...                  ...   \n43787 -0.334972  1.247817    -0.458066       -0.419791             0.694594   \n43788 -0.334972 -0.801399    -0.458066       -0.419791            -0.400374   \n43789 -0.334972  1.247817    -0.458066       -0.419791             0.328889   \n43790 -0.334972  1.247817    -0.458066       -0.419791            -0.417410   \n43791  2.985327 -0.801399    -0.458066       -0.419791            -0.302231   \n\n       followersCount  friendsCount  verified  protected  mediaCount  \\\n0            0.002232     -0.219794 -0.375237        0.0    0.265445   \n1           -0.090949     -0.171491 -0.375237        0.0   -0.207742   \n2            0.162518     -0.139337  2.664982        0.0    0.070213   \n3           -0.089282     -0.053117 -0.375237        0.0    0.151039   \n4           -0.091088     -0.198737 -0.375237        0.0   -0.213268   \n...               ...           ...       ...        ...         ...   \n43787       -0.090397     -0.170140 -0.375237        0.0   -0.063419   \n43788       -0.090949     -0.203077 -0.375237        0.0   -0.210424   \n43789       -0.090079     -0.143320 -0.375237        0.0    0.481865   \n43790       -0.090298     -0.176257 -0.375237        0.0   -0.147809   \n43791       -0.090230     -0.208625 -0.375237        0.0   -0.060869   \n\n       predicted_sentiment  \n0                -0.049500  \n1                -0.049500  \n2                -1.339798  \n3                 1.240799  \n4                -1.339798  \n...                    ...  \n43787             1.240799  \n43788            -1.339798  \n43789            -0.049500  \n43790            -0.049500  \n43791            -1.339798  \n\n[43792 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>links</th>\n      <th>media</th>\n      <th>quotedTweet</th>\n      <th>mentionedUsers</th>\n      <th>UserFavouritesCount</th>\n      <th>followersCount</th>\n      <th>friendsCount</th>\n      <th>verified</th>\n      <th>protected</th>\n      <th>mediaCount</th>\n      <th>predicted_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.985327</td>\n      <td>1.247817</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>-0.537304</td>\n      <td>0.002232</td>\n      <td>-0.219794</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>0.265445</td>\n      <td>-0.049500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.334972</td>\n      <td>1.247817</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>-0.406681</td>\n      <td>-0.090949</td>\n      <td>-0.171491</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>-0.207742</td>\n      <td>-0.049500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.334972</td>\n      <td>-0.801399</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>0.148718</td>\n      <td>0.162518</td>\n      <td>-0.139337</td>\n      <td>2.664982</td>\n      <td>0.0</td>\n      <td>0.070213</td>\n      <td>-1.339798</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.334972</td>\n      <td>-0.801399</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>0.271818</td>\n      <td>-0.089282</td>\n      <td>-0.053117</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>0.151039</td>\n      <td>1.240799</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.334972</td>\n      <td>-0.801399</td>\n      <td>-0.458066</td>\n      <td>2.382140</td>\n      <td>-0.576573</td>\n      <td>-0.091088</td>\n      <td>-0.198737</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>-0.213268</td>\n      <td>-1.339798</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43787</th>\n      <td>-0.334972</td>\n      <td>1.247817</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>0.694594</td>\n      <td>-0.090397</td>\n      <td>-0.170140</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>-0.063419</td>\n      <td>1.240799</td>\n    </tr>\n    <tr>\n      <th>43788</th>\n      <td>-0.334972</td>\n      <td>-0.801399</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>-0.400374</td>\n      <td>-0.090949</td>\n      <td>-0.203077</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>-0.210424</td>\n      <td>-1.339798</td>\n    </tr>\n    <tr>\n      <th>43789</th>\n      <td>-0.334972</td>\n      <td>1.247817</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>0.328889</td>\n      <td>-0.090079</td>\n      <td>-0.143320</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>0.481865</td>\n      <td>-0.049500</td>\n    </tr>\n    <tr>\n      <th>43790</th>\n      <td>-0.334972</td>\n      <td>1.247817</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>-0.417410</td>\n      <td>-0.090298</td>\n      <td>-0.176257</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>-0.147809</td>\n      <td>-0.049500</td>\n    </tr>\n    <tr>\n      <th>43791</th>\n      <td>2.985327</td>\n      <td>-0.801399</td>\n      <td>-0.458066</td>\n      <td>-0.419791</td>\n      <td>-0.302231</td>\n      <td>-0.090230</td>\n      <td>-0.208625</td>\n      <td>-0.375237</td>\n      <td>0.0</td>\n      <td>-0.060869</td>\n      <td>-1.339798</td>\n    </tr>\n  </tbody>\n</table>\n<p>43792 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Generate Other Features as Tensors","metadata":{}},{"cell_type":"code","source":"df_sample = df_data.copy()\ndf_sample['ViralGroup'] = df['ViralGroup']\nprint(df_sample.head(10))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:43.741269Z","iopub.execute_input":"2024-04-22T23:14:43.742022Z","iopub.status.idle":"2024-04-22T23:14:43.756705Z","shell.execute_reply.started":"2024-04-22T23:14:43.741990Z","shell.execute_reply":"2024-04-22T23:14:43.755723Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"   links  media  quotedTweet  mentionedUsers  UserFavouritesCount  \\\n0      1      1            0               0                 5641   \n1      0      1            0               0                18108   \n2      0      0            0               0                71117   \n3      0      0            0               0                82866   \n4      0      0            0               1                 1893   \n5      0      0            0               0                 5351   \n6      0      1            0               1                29471   \n7      0      1            0               0                43758   \n8      0      0            1               0                20790   \n9      0      0            0               1                30810   \n\n   followersCount  friendsCount  verified  protected  mediaCount  \\\n0        143474.0            31         0          0     14731.0   \n1           293.0           710         0          0       259.0   \n2        389770.0          1162         1          0      8760.0   \n3          2854.0          2374         0          0     11232.0   \n4            79.0           327         0          0        90.0   \n5          2235.0           415         0          0        60.0   \n6           289.0          1357         0          0       352.0   \n7         27336.0           340         0          0      1230.0   \n8         25186.0           451         0          0      4581.0   \n9        457456.0          3493         0          0      9173.0   \n\n   predicted_sentiment ViralGroup  \n0                    0       High  \n1                    0        Low  \n2                   -1       High  \n3                    1       High  \n4                   -1        Low  \n5                    1        Low  \n6                    0       High  \n7                   -1       High  \n8                    0        Low  \n9                   -1       High  \n","output_type":"stream"}]},{"cell_type":"code","source":"df_content = pd.DataFrame(df['content'])\ndf_content","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:46.607276Z","iopub.execute_input":"2024-04-22T23:14:46.608025Z","iopub.status.idle":"2024-04-22T23:14:46.619396Z","shell.execute_reply.started":"2024-04-22T23:14:46.607992Z","shell.execute_reply":"2024-04-22T23:14:46.618356Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                 content\n0      Red Velvet releases The ReVe Festival 2022 tea...\n1                         diazin https://t.co/XVx1NQOiwT\n2      venus in virgo squaring mars in gemini reminds...\n3      now that page is attacking me and y’all wanna ...\n4                        @folding_laundry not a bad take\n...                                                  ...\n43787  what they’re about to do is literally life alt...\n43788  @emersonRpage idk what’s up with my luck fr bu...\n43789  How they expect Melissa not to date?  #BlackWi...\n43790  Seems like a fitting end to this Monday. https...\n43791  - Okay I won’t do the pose\\n“No,no. Do it! I’l...\n\n[43792 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Red Velvet releases The ReVe Festival 2022 tea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>diazin https://t.co/XVx1NQOiwT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>venus in virgo squaring mars in gemini reminds...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>now that page is attacking me and y’all wanna ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@folding_laundry not a bad take</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43787</th>\n      <td>what they’re about to do is literally life alt...</td>\n    </tr>\n    <tr>\n      <th>43788</th>\n      <td>@emersonRpage idk what’s up with my luck fr bu...</td>\n    </tr>\n    <tr>\n      <th>43789</th>\n      <td>How they expect Melissa not to date?  #BlackWi...</td>\n    </tr>\n    <tr>\n      <th>43790</th>\n      <td>Seems like a fitting end to this Monday. https...</td>\n    </tr>\n    <tr>\n      <th>43791</th>\n      <td>- Okay I won’t do the pose\\n“No,no. Do it! I’l...</td>\n    </tr>\n  </tbody>\n</table>\n<p>43792 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    # Remove other non-word characters, numbers, etc., if you wish\n    text = re.sub(r'\\W+|\\d+', ' ', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:49.457673Z","iopub.execute_input":"2024-04-22T23:14:49.458036Z","iopub.status.idle":"2024-04-22T23:14:49.463030Z","shell.execute_reply.started":"2024-04-22T23:14:49.458007Z","shell.execute_reply":"2024-04-22T23:14:49.462058Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_content['clean_content'] = df_content['content'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:51.839171Z","iopub.execute_input":"2024-04-22T23:14:51.839499Z","iopub.status.idle":"2024-04-22T23:14:52.889854Z","shell.execute_reply.started":"2024-04-22T23:14:51.839474Z","shell.execute_reply":"2024-04-22T23:14:52.889060Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndf_content['tokenized'] = df_content['clean_content'].apply(lambda x: [word for word in x.split() if word not in (stop)])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:54.299299Z","iopub.execute_input":"2024-04-22T23:14:54.299658Z","iopub.status.idle":"2024-04-22T23:14:55.744221Z","shell.execute_reply.started":"2024-04-22T23:14:54.299632Z","shell.execute_reply":"2024-04-22T23:14:55.743431Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"df_content['cleaned_text'] = df_content['tokenized'].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:57.549561Z","iopub.execute_input":"2024-04-22T23:14:57.549967Z","iopub.status.idle":"2024-04-22T23:14:57.594520Z","shell.execute_reply.started":"2024-04-22T23:14:57.549927Z","shell.execute_reply":"2024-04-22T23:14:57.592975Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=0.001, max_df=0.7, smooth_idf=True)\ntfidf_matrix = vectorizer.fit_transform(df_content['cleaned_text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:59.781817Z","iopub.execute_input":"2024-04-22T23:14:59.782581Z","iopub.status.idle":"2024-04-22T23:15:00.503619Z","shell.execute_reply.started":"2024-04-22T23:14:59.782547Z","shell.execute_reply":"2024-04-22T23:15:00.502845Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tfidf_dense = tfidf_matrix.todense()\nscaler2 = StandardScaler()\nscaled_tfidf = scaler2.fit_transform(np.asarray(tfidf_dense))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:02.023661Z","iopub.execute_input":"2024-04-22T23:15:02.024329Z","iopub.status.idle":"2024-04-22T23:15:03.431782Z","shell.execute_reply.started":"2024-04-22T23:15:02.024296Z","shell.execute_reply":"2024-04-22T23:15:03.430717Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tfidf_tensor = torch.tensor(scaled_tfidf, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:04.429063Z","iopub.execute_input":"2024-04-22T23:15:04.430149Z","iopub.status.idle":"2024-04-22T23:15:04.561290Z","shell.execute_reply.started":"2024-04-22T23:15:04.430110Z","shell.execute_reply":"2024-04-22T23:15:04.560389Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"features = torch.tensor(df_scaled.values, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:06.974351Z","iopub.execute_input":"2024-04-22T23:15:06.975197Z","iopub.status.idle":"2024-04-22T23:15:06.980219Z","shell.execute_reply.started":"2024-04-22T23:15:06.975165Z","shell.execute_reply":"2024-04-22T23:15:06.979218Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"combined_features = torch.cat((features, tfidf_tensor), dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:09.539352Z","iopub.execute_input":"2024-04-22T23:15:09.540082Z","iopub.status.idle":"2024-04-22T23:15:09.668759Z","shell.execute_reply.started":"2024-04-22T23:15:09.540051Z","shell.execute_reply":"2024-04-22T23:15:09.667921Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Encode Virality Labels and Generate Target Tensor for Training","metadata":{}},{"cell_type":"code","source":"mapping = {'High': 0, 'Low': 1}\ndf['ViralGroup_encoded'] = df['ViralGroupAdjusted'].map(mapping)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:13.553779Z","iopub.execute_input":"2024-04-22T23:15:13.554447Z","iopub.status.idle":"2024-04-22T23:15:13.564759Z","shell.execute_reply.started":"2024-04-22T23:15:13.554413Z","shell.execute_reply":"2024-04-22T23:15:13.563743Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"virals = ['ViralGroupAdjusted','ViralGroup_encoded']\ndf[virals]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:15.659796Z","iopub.execute_input":"2024-04-22T23:15:15.660673Z","iopub.status.idle":"2024-04-22T23:15:15.672195Z","shell.execute_reply.started":"2024-04-22T23:15:15.660637Z","shell.execute_reply":"2024-04-22T23:15:15.671242Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"      ViralGroupAdjusted  ViralGroup_encoded\n0                   High                   0\n1                   High                   0\n2                   High                   0\n3                   High                   0\n4                    Low                   1\n...                  ...                 ...\n43787               High                   0\n43788                Low                   1\n43789                Low                   1\n43790               High                   0\n43791               High                   0\n\n[43792 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ViralGroupAdjusted</th>\n      <th>ViralGroup_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Low</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43787</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43788</th>\n      <td>Low</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43789</th>\n      <td>Low</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43790</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43791</th>\n      <td>High</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>43792 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"targets = torch.tensor(df['ViralGroup_encoded'].values)\ntargets = targets.long()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:18.697263Z","iopub.execute_input":"2024-04-22T23:15:18.697900Z","iopub.status.idle":"2024-04-22T23:15:18.705140Z","shell.execute_reply.started":"2024-04-22T23:15:18.697869Z","shell.execute_reply":"2024-04-22T23:15:18.703837Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF","metadata":{}},{"cell_type":"markdown","source":"### Define Neural Network for TF-IDF Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,vocab_size, feature_size, num_classes):\n        super(Model, self).__init__()\n        self.fc_tfidf = nn.Linear(vocab_size, 16)\n        self.relu0 = nn.ReLU()\n        \n        #self.fc1 = nn.Linear(feature_size+16, 64)  # 10 input features\n        self.fc1 = nn.Linear(16, 64)  # 10 input features\n        \n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(64, 64)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(64, num_classes)   # 3 output classes\n\n    def forward(self, tfidf, features):\n        tfidf_out = self.fc_tfidf(tfidf)\n        tfidf_out = self.relu0(tfidf_out)\n        \n        #combined = torch.cat((tfidf_out, features), 1)\n        combined = tfidf_out\n        \n        x = self.fc1(combined)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)  # No activation, raw scores\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = combined_features.shape[1]\nvocab_size = tfidf_tensor.shape[1]\nmodel = Model(vocab_size, input_size, num_classes=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Dataset and Loaders for Training Loop","metadata":{}},{"cell_type":"code","source":"dataset = TensorDataset(tfidf_tensor, combined_features, targets)\n\nclass_counts = [0, 0]  #(Low, High)\nfor _, _, label in dataset:\n    #print(label)\n    class_counts[label] += 1\n\n#weights = [0] * len(dataset)\n#for idx, (_, _, label) in enumerate(dataset):\n#    weights[idx] = 1.0 / class_counts[label]\n    \n# Define the size of train and test sets\ntrain_size = int(0.8 * len(dataset))  # e.g., 80% of the dataset\ntest_size = len(dataset) - train_size  # The remaining 20% of the dataset\n\n# Randomly split the dataset into train and test sets\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create weights for the entire dataset\nfull_weights = [1.0 / class_counts[label] for _,_, label in dataset]\n# Adjust weights for train_dataset after split\ntrain_weights = [full_weights[idx] for idx in train_dataset.indices]\ntest_weights = [full_weights[idx] for idx in test_dataset.indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sampler = WeightedRandomSampler(train_weights, num_samples=len(train_dataset), replacement=True)\ntest_sampler = WeightedRandomSampler(test_weights, num_samples=len(test_dataset), replacement=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\ntest_loader = DataLoader(test_dataset, batch_size=32, sampler=test_sampler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train TF-IDF Neutal Netword Model","metadata":{}},{"cell_type":"code","source":"num_epochs = 5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_predictions = 0\n\n    for tfidf, feature, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        tfidf, feature, labels = tfidf.to(device), feature.to(device), labels.to(device)  # Move to GPU if available\n        optimizer.zero_grad()  # Zero the parameter gradients\n        outputs = model(tfidf, feature)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute loss\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n        \n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_predictions += labels.size(0)\n        correct_predictions += (predicted == labels).sum().item()\n        \n    epoch_loss = running_loss / len(train_loader)\n    epoch_accuracy = correct_predictions / total_predictions\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nval_loss = 0.0\ncorrect_val = 0\ntotal_val = 0\nn_classes = 2\n\n# Initialize all possible class labels\nclass_correct = {i: 0 for i in range(n_classes)}\nclass_total = {i: 0 for i in range(n_classes)}\nfalse_positives = {i: 0 for i in range(n_classes)}\nfalse_negatives = {i: 0 for i in range(n_classes)}\n\n\nwith torch.no_grad():\n    for tfidf, feature, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        tfidf, feature, labels = tfidf.to(device), feature.to(device), labels.to(device)  # Move to GPU if available\n\n        # Forward pass\n        outputs = model(tfidf, feature)\n        loss = criterion(outputs, labels)\n\n        # Calculate validation loss\n        val_loss += loss.item()\n\n        # Calculate validation accuracy\n        _, predicted = torch.max(outputs.data, 1)\n        total_val += labels.size(0)\n        correct_val += (predicted == labels).sum().item()\n\n        # Update metrics\n        for i in range(len(labels)):\n            label = labels[i].item()\n            pred = predicted[i].item()\n\n            if label not in class_correct:\n                class_correct[label] = 0\n                class_total[label] = 0\n                false_positives[label] = 0\n                false_negatives[label] = 0\n\n            if pred == label:\n                class_correct[label] += 1\n            else:\n                false_positives[pred] += 1\n                false_negatives[label] += 1\n\n            class_total[label] += 1\n        \nval_loss /= len(test_loader)\nval_accuracy = correct_val / total_val\n\noverall_precision = sum(class_correct.values()) / (sum(class_correct.values()) + sum(false_positives.values()))\noverall_recall = sum(class_correct.values()) / (sum(class_correct.values()) + sum(false_negatives.values()))\noverall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n\n\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Overall Precision: {overall_precision:.4f}, Overall Recall: {overall_recall:.4f}, Overall F1: {overall_f1:.4f}\")\n\n# Print accuracy for each class\nfor class_id in class_correct:\n    class_acc = 100 * class_correct[class_id] / class_total[class_id]\n    print(f\"Accuracy of Class {class_id}: {class_acc:.2f}%\")\n    \n# Print accuracy, precision, and recall for each class\nfor class_id in class_correct:\n    accuracy = class_correct[class_id] / class_total[class_id] if class_total[class_id] > 0 else 0\n    precision = class_correct[class_id] / (class_correct[class_id] + false_positives[class_id]) if class_correct[class_id] + false_positives[class_id] > 0 else 0\n    recall = class_correct[class_id] / (class_correct[class_id] + false_negatives[class_id]) if class_correct[class_id] + false_negatives[class_id] > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    print(f\"Class {class_id} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"markdown","source":"### Setup BERT Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:28.352483Z","iopub.execute_input":"2024-04-22T23:15:28.353200Z","iopub.status.idle":"2024-04-22T23:15:28.359480Z","shell.execute_reply.started":"2024-04-22T23:15:28.353169Z","shell.execute_reply":"2024-04-22T23:15:28.358694Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_length = 128  # Choose a max_length that suits your dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:30.503676Z","iopub.execute_input":"2024-04-22T23:15:30.504662Z","iopub.status.idle":"2024-04-22T23:15:31.825798Z","shell.execute_reply.started":"2024-04-22T23:15:30.504626Z","shell.execute_reply":"2024-04-22T23:15:31.824996Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99cbcbef6784ab78c3a8dd599a1d227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bc5626b109430b96c261ffa801ce0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662f7da4c71a484aa1fe5f03e84f8188"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0172a60ff7244d88be2d330761403fdf"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = re.sub(r'http\\S+', '[URL]', text)\n    text = re.sub(r'@\\w+', '[MENTION]', text)\n    \n    tokens = tokenizer(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n    return tokens['input_ids'], tokens['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:35.635922Z","iopub.execute_input":"2024-04-22T23:15:35.636311Z","iopub.status.idle":"2024-04-22T23:15:35.641596Z","shell.execute_reply.started":"2024-04-22T23:15:35.636281Z","shell.execute_reply":"2024-04-22T23:15:35.640621Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {'additional_special_tokens': ['[URL]', '[MENTION]']}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:37.784804Z","iopub.execute_input":"2024-04-22T23:15:37.785299Z","iopub.status.idle":"2024-04-22T23:15:37.797661Z","shell.execute_reply.started":"2024-04-22T23:15:37.785268Z","shell.execute_reply":"2024-04-22T23:15:37.796618Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"processed_text = [preprocess_text(text) for text in df_content['content']]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:40.577345Z","iopub.execute_input":"2024-04-22T23:15:40.577677Z","iopub.status.idle":"2024-04-22T23:16:23.254786Z","shell.execute_reply.started":"2024-04-22T23:15:40.577654Z","shell.execute_reply":"2024-04-22T23:16:23.253985Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"input_ids = [item[0] for item in processed_text]\nattention_masks = [item[1] for item in processed_text]\n\ninput_ids = torch.stack(input_ids).squeeze(1)\nattention_masks = torch.stack(attention_masks).squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:27.467598Z","iopub.execute_input":"2024-04-22T23:16:27.468455Z","iopub.status.idle":"2024-04-22T23:16:27.639321Z","shell.execute_reply.started":"2024-04-22T23:16:27.468419Z","shell.execute_reply":"2024-04-22T23:16:27.638118Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(\"input_ids shape:\", input_ids.shape)\nprint(\"attention_masks shape:\", attention_masks.shape)\nprint(\"features shape:\", features.shape)\nprint(\"targets shape:\", targets.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:36.986271Z","iopub.execute_input":"2024-04-22T23:16:36.986979Z","iopub.status.idle":"2024-04-22T23:16:36.992201Z","shell.execute_reply.started":"2024-04-22T23:16:36.986949Z","shell.execute_reply":"2024-04-22T23:16:36.991218Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"input_ids shape: torch.Size([43792, 128])\nattention_masks shape: torch.Size([43792, 128])\nfeatures shape: torch.Size([43792, 11])\ntargets shape: torch.Size([43792])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Define Dataset and Loader based on if doing Text-Only or All-Features","metadata":{}},{"cell_type":"code","source":"dataset = TensorDataset(input_ids, attention_masks, features, targets)\n#dataset = TensorDataset(input_ids, attention_masks, combined_features, targets)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:39.942280Z","iopub.execute_input":"2024-04-22T23:16:39.942668Z","iopub.status.idle":"2024-04-22T23:16:39.947706Z","shell.execute_reply.started":"2024-04-22T23:16:39.942640Z","shell.execute_reply":"2024-04-22T23:16:39.946501Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class_counts = [0, 0]  #(Low, High)\nfor _, _, _, label in dataset:\n    #print(label)\n    class_counts[label] += 1\n\n#weights = [0] * len(dataset)\n#for idx, (_, _, label) in enumerate(dataset):\n#    weights[idx] = 1.0 / class_counts[label]\n    \n# Define the size of train and test sets\ntrain_size = int(0.8 * len(dataset))  # e.g., 80% of the dataset\ntest_size = len(dataset) - train_size  # The remaining 20% of the dataset\n\n# Randomly split the dataset into train and test sets\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create weights for the entire dataset\nfull_weights = [1.0 / class_counts[label] for _,_,_, label in dataset]\n# Adjust weights for train_dataset after split\ntrain_weights = [full_weights[idx] for idx in train_dataset.indices]\ntest_weights = [full_weights[idx] for idx in test_dataset.indices]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:41.657842Z","iopub.execute_input":"2024-04-22T23:16:41.658682Z","iopub.status.idle":"2024-04-22T23:16:42.749474Z","shell.execute_reply.started":"2024-04-22T23:16:41.658652Z","shell.execute_reply":"2024-04-22T23:16:42.748461Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_sampler = WeightedRandomSampler(train_weights, num_samples=len(train_dataset), replacement=True)\ntest_sampler = WeightedRandomSampler(test_weights, num_samples=len(test_dataset), replacement=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\ntest_loader = DataLoader(test_dataset, batch_size=32, sampler=test_sampler)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:45.356548Z","iopub.execute_input":"2024-04-22T23:16:45.356902Z","iopub.status.idle":"2024-04-22T23:16:45.369681Z","shell.execute_reply.started":"2024-04-22T23:16:45.356874Z","shell.execute_reply":"2024-04-22T23:16:45.368615Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Define and Initialize Model","metadata":{}},{"cell_type":"markdown","source":"Model For All Features","metadata":{}},{"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self, bert_model_name, num_other_features, lstm_hidden_size, lstm_layers, num_classes):\n        super(MyModel, self).__init__()\n        # BERT Model\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        bert_output_size = self.bert.config.hidden_size\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size=bert_output_size, hidden_size=lstm_hidden_size, \n                            num_layers=lstm_layers, batch_first=True)\n        # Fully connected layers for combined features\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden_size + num_other_features, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, other_features):\n        # Handling text data with BERT\n        with torch.no_grad():\n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_embeddings = bert_output.last_hidden_state\n\n        # Passing BERT embeddings to LSTM\n        lstm_output, _ = self.lstm(bert_embeddings)\n        # Using only the last hidden state of LSTM\n        lstm_output = lstm_output[:, -1, :]\n\n        # Concatenating LSTM output with other features\n        combined_features = torch.cat((lstm_output, other_features), dim=1)\n\n        # Classifier\n        output = self.classifier(combined_features)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model for Text-Only","metadata":{}},{"cell_type":"code","source":"class MyModel2(nn.Module):\n    def __init__(self, bert_model_name, lstm_hidden_size, lstm_layers, num_classes):\n        super(MyModel2, self).__init__()\n        # BERT Model\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        bert_output_size = self.bert.config.hidden_size\n        # LSTM layer\n        self.lstm = nn.LSTM(input_size=bert_output_size, hidden_size=lstm_hidden_size, \n                            num_layers=lstm_layers, batch_first=True)\n        # Fully connected layers for combined features\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden_size, 64),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Handling text data with BERT\n        with torch.no_grad():\n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_embeddings = bert_output.last_hidden_state\n\n        # Passing BERT embeddings to LSTM\n        lstm_output, _ = self.lstm(bert_embeddings)\n        # Using only the last hidden state of LSTM\n        lstm_output = lstm_output[:, -1, :]\n\n        # Concatenating LSTM output with other features\n        #combined_features = torch.cat((lstm_output), dim=1)\n\n        # Classifier\n        output = self.classifier(lstm_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:49.270619Z","iopub.execute_input":"2024-04-22T23:16:49.271450Z","iopub.status.idle":"2024-04-22T23:16:49.279732Z","shell.execute_reply.started":"2024-04-22T23:16:49.271418Z","shell.execute_reply":"2024-04-22T23:16:49.278791Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"bert_model_name = 'bert-base-uncased'\nnum_other_features = features.shape[1]\n#num_other_features = combined_features.shape[1]\nlstm_hidden_size = 32\nlstm_layers = 2\nnum_classes = 2","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:53.352629Z","iopub.execute_input":"2024-04-22T23:16:53.353626Z","iopub.status.idle":"2024-04-22T23:16:53.357882Z","shell.execute_reply.started":"2024-04-22T23:16:53.353591Z","shell.execute_reply":"2024-04-22T23:16:53.357007Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#lstm_model = MyModel(bert_model_name, num_other_features, lstm_hidden_size, lstm_layers, num_classes)\nlstm_model = MyModel2(bert_model_name, lstm_hidden_size, lstm_layers, num_classes)\nlstm_model.bert.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:55.303806Z","iopub.execute_input":"2024-04-22T23:16:55.304186Z","iopub.status.idle":"2024-04-22T23:16:58.642088Z","shell.execute_reply.started":"2024-04-22T23:16:55.304156Z","shell.execute_reply":"2024-04-22T23:16:58.641051Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6f36e862e5438186c08892e16c3f26"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Embedding(30524, 768)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lstm_model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:16:59.617291Z","iopub.execute_input":"2024-04-22T23:16:59.617650Z","iopub.status.idle":"2024-04-22T23:16:59.624369Z","shell.execute_reply.started":"2024-04-22T23:16:59.617616Z","shell.execute_reply":"2024-04-22T23:16:59.623243Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"best_loss = np.inf","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:17:01.324844Z","iopub.execute_input":"2024-04-22T23:17:01.325715Z","iopub.status.idle":"2024-04-22T23:17:01.329553Z","shell.execute_reply.started":"2024-04-22T23:17:01.325683Z","shell.execute_reply":"2024-04-22T23:17:01.328555Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop, Adjust Epoch Number or Run Multiple Times","metadata":{}},{"cell_type":"code","source":"num_epochs = 1 # Number of epochs\nn_iterations = 1000  # Number of iterations/batches per epoch\nbatch_size = 32  # Your batch size\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlstm_model = lstm_model.to(device)\n\nfor epoch in range(num_epochs):\n    lstm_model.train()\n    running_loss = 0.0\n    batch_count = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with tqdm(total=n_iterations, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n        for i, batch in enumerate(train_loader):\n            #print(\"Batch \",i)\n            if i >= n_iterations:\n                break  # Stop after n_iterations\n            b_input_ids, b_attention_masks, b_other_features, b_labels = [t.to(device) for t in batch]\n            optimizer.zero_grad()  # Zero the parameter gradients\n            #outputs = lstm_model(b_input_ids, b_attention_masks, b_other_features)  # Forward pass\n            outputs = lstm_model(b_input_ids, b_attention_masks)  # Forward pass\n            loss = criterion(outputs, b_labels)  # Compute loss\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update weights\n\n            # Statistics\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_predictions += b_labels.size(0)\n            correct_predictions += (predicted == b_labels).sum().item()\n\n            # Update tqdm bar\n            pbar.update(1)  # This updates the progress bar by one iteration\n            \n            batch_count += 1\n            if batch_count >= n_iterations:\n                break\n        \n    epoch_loss = running_loss / len(train_loader)\n    epoch_accuracy = correct_predictions / total_predictions\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Loop. Update Best Model if Testing Loss decreases","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlstm_model = lstm_model.to(device)\nlstm_model.eval()  # Set the model to evaluation mode\n\ntotal_loss = 0.0\nall_labels = []\nall_predictions = []\nn_classes=num_classes\n\nwith torch.no_grad():  # No need to track gradients during evaluation\n    for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n        b_input_ids, b_attention_masks, b_other_features, b_labels = [t.to(device) for t in batch]\n\n        # Forward pass\n        #outputs = lstm_model(b_input_ids, b_attention_masks, b_other_features)\n        outputs = lstm_model(b_input_ids, b_attention_masks)\n\n        # Compute loss\n        loss = criterion(outputs, b_labels)\n        total_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n\n        all_labels.extend(b_labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\n# Compute overall metrics\noverall_accuracy = accuracy_score(all_labels, all_predictions)\noverall_loss = total_loss / len(test_loader)\noverall_precision = precision_score(all_labels, all_predictions)\noverall_recall = recall_score(all_labels, all_predictions)\noverall_f1 = f1_score(all_labels, all_predictions)\n\n# Compute class-wise metrics\nclass_accuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\n\nfor i in range(n_classes):\n    labels_i = np.array(all_labels) == i\n    predictions_i = np.array(all_predictions) == i\n\n    class_accuracies.append(accuracy_score(labels_i, predictions_i))\n    precisions.append(precision_score(labels_i, predictions_i, zero_division=0))\n    recalls.append(recall_score(labels_i, predictions_i, zero_division=0))\n    f1_scores.append(f1_score(labels_i, predictions_i, zero_division=0))\n\n# Display results\nprint(f\"Overall Loss: {overall_loss:.4f}, Overall Accuracy: {overall_accuracy:.4f}\")\nprint(f\"Overall Precision: {overall_precision:.4f}, Overall Recall: {overall_recall:.4f}, Overall F1: {overall_f1:.4f}\")\nprint(f\"Class Accuracies: {class_accuracies}\")\nprint(f\"Precision per Class: {precisions}\")\nprint(f\"Recall per Class: {recalls}\")\nprint(f\"F1 Score per Class: {f1_scores}\")\n\n#Update Best Model\nif overall_loss < best_loss:\n    print(\"Updating Best Model\")\n    best_model = lstm_model\n    best_loss = overall_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model = best_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = lstm_model\nbest_loss = overall_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(lstm_model, '/kaggle/working/model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model = torch.load('/kaggle/input/twitter-data-virality/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:40.092192Z","iopub.execute_input":"2024-04-22T20:52:40.092926Z","iopub.status.idle":"2024-04-22T20:52:44.280880Z","shell.execute_reply.started":"2024-04-22T20:52:40.092891Z","shell.execute_reply":"2024-04-22T20:52:44.279618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LIME","metadata":{}},{"cell_type":"markdown","source":"### Use LIME to get word importance for each input","metadata":{}},{"cell_type":"code","source":"device = \"cuda\"\nlstm_model.to(device)\nlstm_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:46.762728Z","iopub.execute_input":"2024-04-22T20:52:46.763167Z","iopub.status.idle":"2024-04-22T20:52:46.853990Z","shell.execute_reply.started":"2024-04-22T20:52:46.763132Z","shell.execute_reply":"2024-04-22T20:52:46.852941Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_proba(texts, batch_size=32):\n    # List to hold all probabilities\n    all_probabilities = []\n    processed_texts = []\n    \n    for text in texts:\n        text = re.sub(r'http\\S+', '[URL]', text)  # Replace URLs with '[URL]'\n        text = re.sub(r'@\\w+', '[MENTION]', text)  # Replace mentions with '[MENTION]'\n        processed_texts.append(text)\n    \n    # Process texts in batches\n    for i in range(0, len(processed_texts), batch_size):\n        batch_texts = processed_texts[i:i + batch_size]\n        inputs = tokenizer(batch_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n\n        # Move to device\n        device = next(lstm_model.parameters()).device\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n\n        # Model prediction\n        with torch.no_grad():\n            logits = lstm_model(input_ids, attention_mask)\n\n        # Convert logits to probabilities\n        probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()\n        all_probabilities.extend(probabilities)\n\n        # Memory cleanup\n        del input_ids, attention_mask, logits\n        torch.cuda.empty_cache()\n    all_probabilities = np.array(all_probabilities)\n    return all_probabilities","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:50.744120Z","iopub.execute_input":"2024-04-22T20:52:50.745139Z","iopub.status.idle":"2024-04-22T20:52:50.754982Z","shell.execute_reply.started":"2024-04-22T20:52:50.745101Z","shell.execute_reply":"2024-04-22T20:52:50.753799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=['High', 'Low'])  # Customize class names based on your specific case","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:52.665077Z","iopub.execute_input":"2024-04-22T20:52:52.665768Z","iopub.status.idle":"2024-04-22T20:52:52.673026Z","shell.execute_reply.started":"2024-04-22T20:52:52.665710Z","shell.execute_reply":"2024-04-22T20:52:52.671817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_row = df.sample(n=1)\nr_content = random_row['content'].iloc[0]\nrandom_row","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:54.132732Z","iopub.execute_input":"2024-04-22T20:52:54.133142Z","iopub.status.idle":"2024-04-22T20:52:54.159433Z","shell.execute_reply.started":"2024-04-22T20:52:54.133114Z","shell.execute_reply":"2024-04-22T20:52:54.158290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_input_id, r_attention = preprocess_text(r_content)\nlstm_model.eval()\nr_input_id, r_attention = r_input_id.to(device), r_attention.to(device)\nr_outputs = lstm_model(r_input_id, r_attention)\n_, r_predicted = torch.max(r_outputs, 1)\nprint('Outputs: ',r_outputs)\nprint('Prediction: ',r_predicted)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:56.137131Z","iopub.execute_input":"2024-04-22T20:52:56.138061Z","iopub.status.idle":"2024-04-22T20:52:56.880660Z","shell.execute_reply.started":"2024-04-22T20:52:56.138017Z","shell.execute_reply":"2024-04-22T20:52:56.879526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = r_outputs\nprobabilities = torch.nn.functional.softmax(logits, dim=-1)\nprint(probabilities)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:58.405042Z","iopub.execute_input":"2024-04-22T20:52:58.405409Z","iopub.status.idle":"2024-04-22T20:52:58.413303Z","shell.execute_reply.started":"2024-04-22T20:52:58.405380Z","shell.execute_reply":"2024-04-22T20:52:58.412279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = r_content","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:53:00.242961Z","iopub.execute_input":"2024-04-22T20:53:00.243369Z","iopub.status.idle":"2024-04-22T20:53:00.248074Z","shell.execute_reply.started":"2024-04-22T20:53:00.243337Z","shell.execute_reply":"2024-04-22T20:53:00.247071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explanation = explainer.explain_instance(sample_text, predict_proba, num_features=20, num_samples=500)\n\n# Visualize the explanation\nexplanation.show_in_notebook(text=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:53:10.261278Z","iopub.execute_input":"2024-04-22T20:53:10.261975Z","iopub.status.idle":"2024-04-22T20:53:12.493229Z","shell.execute_reply.started":"2024-04-22T20:53:10.261940Z","shell.execute_reply":"2024-04-22T20:53:12.492072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggregated LIME","metadata":{"execution":{"iopub.status.busy":"2024-04-12T22:59:24.538917Z","iopub.execute_input":"2024-04-12T22:59:24.539294Z","iopub.status.idle":"2024-04-12T22:59:24.623029Z","shell.execute_reply.started":"2024-04-12T22:59:24.539266Z","shell.execute_reply":"2024-04-12T22:59:24.621660Z"}}},{"cell_type":"code","source":"from collections import defaultdict\nfrom itertools import islice\nimport nltk\n\n\nnum_iterations = min(1000, len(df['content']))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:55:02.306876Z","iopub.execute_input":"2024-04-22T20:55:02.307247Z","iopub.status.idle":"2024-04-22T20:55:02.312816Z","shell.execute_reply.started":"2024-04-22T20:55:02.307219Z","shell.execute_reply":"2024-04-22T20:55:02.311724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_documents = len(df['content'])\nmin_df = 0.05 * total_documents  # Minimum document frequency to be considered\nmax_df = 0.8 * total_documents  # Maximum frequency allowed before filtering","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:05:01.000155Z","iopub.execute_input":"2024-04-22T21:05:01.000606Z","iopub.status.idle":"2024-04-22T21:05:01.006224Z","shell.execute_reply.started":"2024-04-22T21:05:01.000565Z","shell.execute_reply":"2024-04-22T21:05:01.005160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_occurrences = defaultdict(int)\nword_importance = defaultdict(float)\n\nfor text in tqdm(islice(df['content'], num_iterations), total=num_iterations):\n    explanation = explainer.explain_instance(text, predict_proba, num_features=50, num_samples=100)\n    for word, score in explanation.as_list():\n        word_importance[word] += score\n        token_occurrences[word] += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:55:08.329084Z","iopub.execute_input":"2024-04-22T20:55:08.329754Z","iopub.status.idle":"2024-04-22T21:02:49.058683Z","shell.execute_reply.started":"2024-04-22T20:55:08.329706Z","shell.execute_reply":"2024-04-22T21:02:49.057562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To average the scores\nfor word in word_importance:\n    word_importance[word] /= num_iterations\n    if token_occurrences[word] > 0:  # Ensure there is no division by zero\n        word_importance[word] /= token_occurrences[word]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:02:53.322400Z","iopub.execute_input":"2024-04-22T21:02:53.322861Z","iopub.status.idle":"2024-04-22T21:02:53.333967Z","shell.execute_reply.started":"2024-04-22T21:02:53.322820Z","shell.execute_reply":"2024-04-22T21:02:53.332919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adjusted_word_importance = {}\nfor token, importance in word_importance.items():\n    occurrence = token_occurrences[token]\n    if occurrence < min_df or occurrence > max_df:\n        adjusted_importance = 0\n    else:\n        adjusted_importance = importance\n    adjusted_word_importance[token] = adjusted_importance / num_iterations\n    \nhigh_word_importance = {}\nfor token, importance in adjusted_word_importance.items():\n    high_word_importance[token] = importance*-1","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:02:54.833130Z","iopub.execute_input":"2024-04-22T21:02:54.833693Z","iopub.status.idle":"2024-04-22T21:02:54.849347Z","shell.execute_reply.started":"2024-04-22T21:02:54.833651Z","shell.execute_reply":"2024-04-22T21:02:54.848203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the words by importance\nsorted_words = sorted(high_word_importance.items(), key=lambda x: x[1], reverse=True)\n\n# Split the words and their importance scores\nwords, importances = zip(*sorted_words[:50])  # Top 50 words\n\n# Create bar chart\nplt.figure(figsize=(10, 8))\nplt.barh(words, importances, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Top 20 Word Importances')\nplt.gca().invert_yaxis()  # Invert the y-axis to have the highest scores at the top\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:05:03.394792Z","iopub.execute_input":"2024-04-22T21:05:03.395688Z","iopub.status.idle":"2024-04-22T21:05:04.127024Z","shell.execute_reply.started":"2024-04-22T21:05:03.395654Z","shell.execute_reply":"2024-04-22T21:05:04.126097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nword_score = {word: score for word, score in sorted_words}\nwordcloud = WordCloud(width = 800, height = 400, background_color ='white')\n\nwordcloud.generate_from_frequencies(word_score)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:05:22.156777Z","iopub.execute_input":"2024-04-22T21:05:22.157547Z","iopub.status.idle":"2024-04-22T21:05:22.563155Z","shell.execute_reply.started":"2024-04-22T21:05:22.157509Z","shell.execute_reply":"2024-04-22T21:05:22.562112Z"},"trusted":true},"execution_count":null,"outputs":[]}]}